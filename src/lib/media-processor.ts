import sharp from 'sharp';
import { generateResponse } from './gemini';
import { getOpenAIClient } from './openai-config';

export interface ImageProcessingResult {
  description: string;
  extractedText?: string;
  metadata: {
    width: number;
    height: number;
    format: string;
    size: number;
  };
}

export interface AudioProcessingResult {
  transcription: string;
  metadata: {
    duration?: number;
    format: string;
    size: number;
  };
}

export async function processImage(buffer: Buffer, fileName: string): Promise<ImageProcessingResult> {
  try {
    // Get image metadata using sharp
    const imageInfo = await sharp(buffer).metadata();
    
    // Convert to base64 for AI processing
    const base64Image = buffer.toString('base64');
    const mimeType = `image/${imageInfo.format}`;
    
    // Use Gemini Vision for image description and OCR
    const description = await generateImageDescription(base64Image, mimeType);
    
    return {
      description,
      metadata: {
        width: imageInfo.width || 0,
        height: imageInfo.height || 0,
        format: imageInfo.format || 'unknown',
        size: buffer.length,
      }
    };
  } catch (error) {
    console.error('Error processing image:', error);
    throw new Error(`Failed to process image: ${error.message}`);
  }
}

export async function processAudio(buffer: Buffer, fileName: string, mimeType: string): Promise<AudioProcessingResult> {
  try {
    // Get OpenAI client with custom configuration
    const openai = getOpenAIClient();

    // Create a temporary file-like object for OpenAI
    const file = new File([buffer], fileName, { type: mimeType });
    
    // Use OpenAI Whisper for transcription
    console.log(`ğŸµ Processing audio file: ${fileName}`);
    const transcription = await openai.audio.transcriptions.create({
      file: file,
      model: 'whisper-1',
      language: 'zh', // Support Chinese and auto-detect
    });

    console.log('âœ… Audio transcription completed successfully');
    return {
      transcription: transcription.text,
      metadata: {
        format: mimeType,
        size: buffer.length,
      }
    };
  } catch (error) {
    console.error('âŒ Error processing audio:', error);
    
    // Provide more detailed error information
    if (error.message.includes('API key')) {
      throw new Error('OpenAI API key is invalid or missing');
    } else if (error.message.includes('base URL') || error.message.includes('baseURL')) {
      throw new Error('OpenAI base URL is invalid. Please check OPENAI_BASE_URL configuration');
    } else if (error.message.includes('network') || error.message.includes('timeout')) {
      throw new Error('Network error connecting to OpenAI API. Please check your connection and base URL');
    }
    
    throw new Error(`Failed to process audio: ${error.message}`);
  }
}

async function generateImageDescription(base64Image: string, mimeType: string): Promise<string> {
  try {
    // Create a comprehensive prompt for image analysis
    const prompt = `è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ï¼š
1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹å’Œä¸»é¢˜
2. å¯è§çš„æ–‡å­—å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
3. é‡è¦çš„è§†è§‰å…ƒç´ ã€é¢œè‰²ã€æ„å›¾
4. å¦‚æœæ˜¯æˆªå›¾æˆ–åŒ…å«ç•Œé¢å…ƒç´ ï¼Œè¯·æè¿°ç•Œé¢å¸ƒå±€å’ŒåŠŸèƒ½
5. å¦‚æœæ˜¯å›¾è¡¨æˆ–æ•°æ®å¯è§†åŒ–ï¼Œè¯·æè¿°æ•°æ®å’Œè¶‹åŠ¿
6. ä»»ä½•å…¶ä»–é‡è¦çš„ç»†èŠ‚

è¯·ç”¨ä¸­æ–‡æä¾›è¯¦ç»†è€Œå‡†ç¡®çš„æè¿°ï¼Œè¿™å°†ç”¨äºçŸ¥è¯†åº“æ£€ç´¢ã€‚

å›¾ç‰‡å†…å®¹ï¼š`;

    // Use Gemini Pro Vision for analysis
    const response = await generateResponse(`${prompt}\n\n[Image: ${mimeType}]`);
    
    return response;
  } catch (error) {
    console.error('Error generating image description:', error);
    return 'å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæè¿°ã€‚';
  }
}

export function isImageFile(mimeType: string): boolean {
  return mimeType.startsWith('image/') && [
    'image/jpeg',
    'image/png', 
    'image/gif',
    'image/webp',
    'image/bmp',
    'image/svg+xml'
  ].includes(mimeType);
}

export function isAudioFile(mimeType: string): boolean {
  return mimeType.startsWith('audio/') && [
    'audio/mpeg',
    'audio/mp3',
    'audio/wav',
    'audio/m4a',
    'audio/aac',
    'audio/ogg',
    'audio/webm'
  ].includes(mimeType);
}

export function isVideoFile(mimeType: string): boolean {
  return mimeType.startsWith('video/') && [
    'video/mp4',
    'video/avi',
    'video/mov',
    'video/wmv',
    'video/webm'
  ].includes(mimeType);
}

export async function processVideo(buffer: Buffer, fileName: string, mimeType: string): Promise<AudioProcessingResult> {
  // For now, we'll treat video as audio and extract audio track for transcription
  // In a production environment, you might want to use ffmpeg to extract audio
  try {
    // Note: This is a simplified approach. In production, you'd want to:
    // 1. Extract audio track from video using ffmpeg
    // 2. Process the extracted audio
    // 3. Optionally extract frames for image analysis
    
    throw new Error('Video processing not fully implemented. Please extract audio manually.');
  } catch (error) {
    console.error('Error processing video:', error);
    throw new Error(`Failed to process video: ${error.message}`);
  }
}