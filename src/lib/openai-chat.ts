import { getOpenAIClient } from './openai-config';

interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface ChatResponse {
  message: string;
  sources: string[];
}

interface SearchResult {
  metadata: {
    source: string;
    content: string;
  };
  score?: number;
}

// Generate embeddings using OpenAI
async function generateOpenAIEmbedding(text: string): Promise<number[]> {
  const openai = getOpenAIClient();
  
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text,
  });
  
  return response.data[0].embedding;
}

// Search for similar documents using OpenAI embeddings
async function searchSimilarDocumentsWithOpenAI(query: string): Promise<SearchResult[]> {
  try {
    // Generate embedding for the query
    const queryEmbedding = await generateOpenAIEmbedding(query);
    
    // For now, return empty array since we need to implement Pinecone search with OpenAI embeddings
    // This is a placeholder - the actual implementation would search Pinecone with the OpenAI embedding
    console.log('ğŸ” Generated OpenAI embedding for query:', query.substring(0, 50) + '...');
    console.log('ğŸ“Š Embedding dimensions:', queryEmbedding.length);
    
    // TODO: Implement Pinecone search with OpenAI embeddings
    return [];
  } catch (error) {
    console.error('âŒ Error searching with OpenAI embeddings:', error);
    return [];
  }
}

export async function generateOpenAIResponse(
  message: string,
  history: Message[] = []
): Promise<ChatResponse> {
  try {
    console.log('ğŸ¤– Generating OpenAI response for:', message.substring(0, 100) + '...');
    
    const openai = getOpenAIClient();
    
    // Search for relevant documents using OpenAI embeddings
    const searchResults = await searchSimilarDocumentsWithOpenAI(message);
    console.log(`ğŸ“š Found ${searchResults.length} relevant documents`);
    
    // Prepare context from search results
    const context = searchResults.length > 0 
      ? searchResults.map(result => `æ¥æº: ${result.metadata.source}\nå†…å®¹: ${result.metadata.content}`).join('\n\n')
      : '';
    
    // Prepare system message with RAG context
    const systemMessage = context 
      ? `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç»“åˆæ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜å¹¶åŸºäºä½ çš„çŸ¥è¯†ç»™å‡ºæœ€ä½³å»ºè®®ã€‚

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
${context}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶åœ¨é€‚å½“æ—¶å¼•ç”¨ç›¸å…³æ¥æºã€‚`
      : 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚';

    // Prepare conversation history
    const messages: Message[] = [
      { role: 'system', content: systemMessage },
      ...history.slice(-6), // Keep last 6 messages for context
      { role: 'user', content: message }
    ];

    console.log('ğŸ“¤ Sending request to OpenAI...');
    
    // Generate response using OpenAI
    const completion = await openai.chat.completions.create({
      model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
      messages,
      temperature: parseFloat(process.env.OPENAI_TEMPERATURE || '0.7'),
      max_tokens: parseInt(process.env.OPENAI_MAX_TOKENS || '2000'),
      top_p: parseFloat(process.env.OPENAI_TOP_P || '0.9'),
    });

    const response = completion.choices[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚';
    
    // Extract sources from search results
    const sources = searchResults.map(result => result.metadata.source || 'Unknown source');
    
    console.log('âœ… OpenAI response generated successfully');
    
    return {
      message: response,
      sources: [...new Set(sources)] // Remove duplicates
    };

  } catch (error) {
    console.error('âŒ Error generating OpenAI response:', error);
    
    // Fallback response without RAG
    try {
      const openai = getOpenAIClient();
      const completion = await openai.chat.completions.create({
        model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
        messages: [
          { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚' },
          { role: 'user', content: message }
        ],
        temperature: 0.7,
        max_tokens: 1000,
      });

      const response = completion.choices[0]?.message?.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚';
      
      return {
        message: response,
        sources: []
      };
    } catch (fallbackError) {
      console.error('âŒ Fallback OpenAI request also failed:', fallbackError);
      throw new Error('OpenAI service is currently unavailable');
    }
  }
}

export async function testOpenAIChat(): Promise<{ success: boolean; message: string; model?: string }> {
  try {
    console.log('ğŸ§ª Testing OpenAI chat functionality...');
    
    const openai = getOpenAIClient();
    const testMessage = 'è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚';
    
    const completion = await openai.chat.completions.create({
      model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
      messages: [
        { role: 'user', content: testMessage }
      ],
      max_tokens: 100,
      temperature: 0.7,
    });

    const response = completion.choices[0]?.message?.content;
    
    if (response) {
      console.log('âœ… OpenAI chat test successful');
      return {
        success: true,
        message: 'OpenAIèŠå¤©åŠŸèƒ½æ­£å¸¸',
        model: completion.model || process.env.OPENAI_MODEL || 'gpt-4o-mini'
      };
    } else {
      throw new Error('No response from OpenAI');
    }

  } catch (error) {
    console.error('âŒ OpenAI chat test failed:', error);
    const message = error instanceof Error ? error.message : 'Unknown error';
    return {
      success: false,
      message: `OpenAIèŠå¤©æµ‹è¯•å¤±è´¥: ${message}`
    };
  }
}